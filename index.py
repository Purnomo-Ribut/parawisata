# -*- coding: utf-8 -*-
"""parawisata.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1htjmO-io-3GaPFUf5LY2XE1RP40kFq6Z
"""

#from google.colab import drive
#drive.mount('/content/drive')

#from google.colab import drive
#drive.mount('/content/drive')

#install library
#Modul Library
import streamlit as st
import numpy as np
import pandas as pd
import re



#Modul library Metode 
#from sklearn.naive_bayes import GaussianNB
#from sklearn.neighbors import KNeighborsClassifier
#from sklearn.tree import DecisionTreeClassifier
def load_dataset():
	data = pd.read_csv("destinasi wisata madura - Sheet1.csv")	
	return data

option = st.sidebar.selectbox(
    'Silakan pilih:',
    ('Dataset','Modeling')
)

if option == 'Dataset' or option == '':
    st.write("""# Dataset""") #menampilkan halaman utama
    st.write (""" 
                Purnomo Ribut | 200411100156
		
                Indra Ramadan Fadilafani | 200411100158 """)
    st.write("""
                Dataset yang kami gunakan dalam penelitian ini dengan cara scrawling data di google dengan
website Mytrip dan website random lainnya. dalam website ini kami mengambil nama wisata dan penjelasan wisata
tersebut. berikut data yang kami ambil : 
    """)
    st.dataframe(load_dataset())
    #data = pd.read_csv("destinasi wisata madura - Sheet1.csv")
    # df.rename(columns={"d4r55":"Username","wiI7pd":"Ulasan"}, inplace=True)
    #data = data.drop('no', axis=1)
    st.write (""" 
    Dataset ini terdiri dari 3 fitur yaitu :
    * nama berfungsi untuk menampung nama tempat Wisata
    * penjelasan berfungsi untuk menampung paragraf yang menjelaskan tempat wisata 
    * label berfungsi untuk melabeli apakah wisata termasuk 'Alam' atau 'Buatan'
    
    Dari dataset tersebut kami lakukan Prepocessing data yang terdiri dari : 
    * Cleaning, Cleaning merupakan proses pembersihan data dari karakter yang tidak relevaN seperti tanda baca, mention, character hashtag atau URL. Tujuan dari tahap ini adalah untuk membuat data lebih mudah diolah dan menghilangkan noise pada data. Cleaning text menggunakan library re dan pandas.

    * Case Folding, Case Folding merupakan proses untuk mengkonversi teks ke dalam format huruf kecil (lowercase). Hal ini bertujuan untuk memberikan bentuk standar pada teks.

    * Tokenizing, proses pemotongan teks menjadi bagian-bagian yang lebih kecil, yang disebu token. Pada proses ini juga dilakukan penghilangan angka, tanda baca dan karakter lain yang dianggap tidak memiliki pengaruh terhadap pemrosesan teks.

    * Stopword Removal, Tahap Filtering atau Stopword Removal adalah tahap pemilihan kata-kata yang dianggap penting.

    * Stemming, Stemming adalah proses pengubahan bentuk kata menjadi kata dasar atau tahap mencari root dari tiap kata.
    
    Dilanjutkan dengan Pembobotan data, 
    * Pembobotan kata yang dilakukan adalah kata yang terdapat dalam teks akan diberi berat atau bobot dengan menggunakan metode TF-IDF (Term Frequency - Inverse Document Frequency). Istilah yang terdapat pada satu dokumen lebih difokuskan pada TF (Term Frequency) sedangkan IDF(Inverse Document Frequency) lebih difokuskan pada istilah di banyak dokumen. Metode TF-IDF menggabungkan dua cara untuk perhitungan bobotnya, yaitu dengan menghitung frekuensi kemunculan kata di sebuah dokumen tertentu (TF) dan melakukan perhitungan invers terhadap frekuensi dokumen yang mengandung kata tersebut (IDF).Operator yang digunakan dalam menghubungkan operator dengan data tabel menggunakan operator nominal to text karena dalam Process Documents from Data harus bersifat teks sedangkan data pada tabel bersifat polynomial.
    
    Selanjutnya di Modelling 
    * Dalam tahapan ini Naive Bayes Classification merupakan teknik klasifikasi berdasarkan Teorema Bayes dengan asumsi independensi di antara para prediktor. Naive Bayes Classifier memprediksi peluang di masa depan berdasarkan pengalaman di masa sebelumnya sehingga dikenal sebagai Teorema Bayes. Dalam istilah sederhana, penggolongan Naive Bayes menganggap bahwa kehadiran fitur tertentu di kelas tidak terkait dengan kehadiran fitur lainnya (Hidayatullah, 2014) . Keuntungan penggunan adalah bahwa metode ini hanya membutuhkan jumlah data pelatihan (training data) yang kecil untuk menentukan estimasi parameter yg diperlukan dalam proses pengklasifikasian. Karena yang diasumsikan sebagai variabel independent, maka hanya varians dari suatu variabel dalam sebuah kelas yang dibutuhkan untuk menentukan klasifikasi, bukan keseluruhan dari matriks kovarians. Pada tahapan ini dilakukan modelling dengan model naïve bayes dari data latih yang sudah di pre processing sebelumnya dan diuji dengan data uji

    Selanjutnya Klasifikasi 
    * Pada tahapan proses ini dilakukan untuk menentukan kelas pada masing-masing objek wisata apakah termasuk wisata alam atau buatan dengan menerapkan metode Naive Naive baiyes
    

    """)
    
    
elif option == 'Modeling':
	st.write("""## Modeling dan Implementasi Naive Bayes""") #menampilkan judul halaman dataframe
	st.write("""
	Naïve bayes adalah algoritma yang dikembangkan berdasarkan teorema bayes yang mengasumsikan setiap atribut sebagai independen sendiri dan berbeda dengan atribut lainnya. menggunakan metode Naive Bayes untuk membantu wisatawan dalam memilih objek wisata yang sesuai dengan preferensi mereka. Metode Naive Bayes dipilih karena telah terbukti efektif dalam melakukan klasifikasi pada data dengan fitur yang kompleks seperti halnya pada klasifikasi objek wisata.
	
	""")
	code = '''
	# Membaca file CSV "destinasi wisata madura - Sheet1.csv" dan menyimpan datanya ke dalam variabel `data`
	data = pd.read_csv("destinasi wisata madura - Sheet1.csv")

	# Menghapus kolom 'no' dari DataFrame `data`
	data = data.drop('no', axis=1)

	# Menghitung jumlah nilai null (kosong) dalam setiap kolom DataFrame `data`
	data.isnull().sum()

	# Menampilkan informasi tentang DataFrame `data`, termasuk jumlah baris, jumlah kolom, dan tipe data setiap kolom
	data.info()

	# Menghapus baris yang mengandung nilai null dari DataFrame `data`
	data.dropna(inplace=True)

	# Kembali menghitung jumlah nilai null (kosong) dalam setiap kolom DataFrame `data` setelah baris-baris yang mengandung nilai null dihapus
	data.isnull().sum()

	# Menghitung jumlah kemunculan setiap nilai dalam kolom 'label' DataFrame `data` dan menampilkannya
	data["label"].value_counts()
	'''

	st.code(code, language='python')
	
	data = pd.read_csv("destinasi wisata madura - Sheet1.csv")
	data = data.drop('no', axis=1)
	data.isnull().sum()
	data.info()
	data.dropna(inplace=True)
	data.isnull().sum()
	data["label"].value_counts()
	
	"""## Preprocessing"""	
	def delete_char(text):
	  text = text.replace('\\t',"").replace('\\n',"").replace('\\u',"").replace('\\',"")
	  text = text.encode('ascii', 'replace').decode('ascii')
	  return text.replace("http://"," ").replace("https://", " ")
	  return text.replace("https://","").replace("http://","")
	data["penjelasan"]=data["penjelasan"].apply(delete_char)
	st.write(""" Hasil Cleaning """)	
	data
	
	#hapus angka
	def del_num(text):
	  text =re.sub("\d+","",text)
	  return text
	data["penjelasan"]=data["penjelasan"].apply(del_num)
	st.write(""" Hasil Hapus Angka """)	
	data
	
	#ubah huruf kecil
	def change_var(text):
	  text = text.lower()
	  return text
	data["penjelasan"]=data["penjelasan"].apply(change_var)
	st.write(""" Hasil Ubah Huruf Kecil """)
	data
	
	#hapus tanda hubung
	def remove_punctuation(text):
	  text = re.sub("(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)"," ",text)
	  return text
	data["penjelasan"]=data["penjelasan"].apply(remove_punctuation)
	st.write(""" Hasil Hapus Tanda Hubung """)
	data
	
	
	from string import punctuation	
	import nltk
	nltk.download("punkt")

	from nltk.tokenize import word_tokenize
	data["hasil"]=data["penjelasan"].apply(lambda x: nltk.word_tokenize(x))
	st.write(""" Hasil Tokenizing """)
	data["hasil"]
	

	normalize = pd.read_excel("Normalization Data.xlsx")
	normalize_word_dict={}

	for row in normalize.iterrows():
	  if row[0] not in normalize_word_dict:
	    normalize_word_dict[row[0]] = row[1]

	def normalized_term(comment):
	  return [normalize_word_dict[term] if term in normalize_word_dict else term for term in comment]

	data['comment_normalize'] = data['hasil'].apply(normalized_term)
	data['comment_normalize'].head(10)

	#stopword removal
	nltk.download("stopwords")
	from nltk.corpus import stopwords
	txt_stopwords = stopwords.words("indonesian")

	def stopword_removal(filter):
	  filter = [word for word in filter if word not in txt_stopwords]
	  return filter
	data["stopwords_removal"]=data["comment_normalize"].apply(stopword_removal)
	st.write(""" Hasil stopwords removal Pertama""")
	data["stopwords_removal"]

	#removal2
	data_stopwords=pd.read_excel("list_stopwords.xlsx")

	def stopword_removal2 (filter):
	  filter =[word for word in filter if word not in data_stopwords]
	  return filter
	data["stopwords_removal_final"]=data["stopwords_removal"].apply(stopword_removal2)
	st.write(""" Hasil stopwords removal Final""")
	data["stopwords_removal_final"]

	"""#Proses Stemming"""

	#proses stem
	from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
	import string
	import swifter
	factory = StemmerFactory()
	stemmer = factory.create_stemmer()

	def stemming (term):
	  return stemmer.stem(term)

	term_dict = {}
	for document in data['stopwords_removal_final']:
	  for term in document:
	    if term not in term_dict:
	      term_dict[term] = ''	

	for term in term_dict:
	  term_dict[term] = stemming(term)
	  print(term,":",term_dict[term])
	st.write(term_dict)

	def get_stemming(document):
	  return [term_dict[term] for term in document]

	data['stemming'] = data['stopwords_removal_final'].swifter.apply(get_stemming)

	print(data['stemming'])
	st.write("Hasil Stemming")
	data.head(20)

	#Perhitungan TF-IDF
	def joinkata(data):
	  kalimat = ""
	  for i in data:
	    kalimat += i
	    kalimat += " "
	  return kalimat

	text = data['stemming'].swifter.apply(joinkata)
	st.write("Hasil Perhitungan TF-IDF")
	text
	
	"""#Modeling Naive Bayes """
	from sklearn.feature_extraction.text import TfidfVectorizer
	tfidf_vectorizer = TfidfVectorizer()
	tfidf_separate = tfidf_vectorizer.fit_transform(text)

	df_tfidf = pd.DataFrame(
	    tfidf_separate.toarray(), columns=tfidf_vectorizer.get_feature_names_out(), index=data.index
	)
	X = df_tfidf.values
	Y = data['label']
	df_tfidf
	
	st.write("Data Data Label")
	Y
	
	from sklearn.model_selection import train_test_split
	X_train, X_test, Y_train, Y_test = train_test_split( X, Y, test_size = 0.3, random_state = 100)
	st.write("Pembagian Data Training dan Testing)
	st.write("Jumlah Data training : ", len(X_train))
	st.write("Jumlah Data test : ", len(X_test))

	from sklearn.naive_bayes import GaussianNB
	gnb_model = GaussianNB()
	gnb_model.fit(X_train, Y_train)
	Y_pred = gnb_model.predict(X_test)
	# Menggabungkan X_test dan Y_pred menjadi satu array dua dimensi
	data_pred = list(zip(X_test, Y_pred))

	# Membuat dataframe hasil prediksi
	df_pred = pd.DataFrame(data_pred, columns=['Text', 'Label Prediksi'])
	# Menampilkan dataframe
	st.write("Tabel Prediksi Label")
	st.write(df_pred)
	
	
	from sklearn.metrics import accuracy_score
	Y_pred = gnb_model.predict(X_test)
	st.write(" GNB Accuracy : ",accuracy_score(Y_test,Y_pred)*100)

    	# Menampilkan judul halaman
	st.write("Implementasi")

	# Menerima input teks dari pengguna
	input_text = st.text_input("Masukkan teks untuk diprediksi")

	# Tombol untuk memulai prediksi
	if st.button("Prediksi"):
	    # Melakukan preprocessing pada teks inputan
	    input_text = delete_char(input_text)
	    input_text = del_num(input_text)
	    input_text = change_var(input_text)
	    input_text = remove_punctuation(input_text)
	    input_text = nltk.word_tokenize(input_text)
	    input_text = normalized_term(input_text)
	    input_text = stopword_removal(input_text)
	    input_text = stopword_removal2(input_text)
	    input_text = get_stemming(input_text)
	    input_text = joinkata(input_text)

	    # Melakukan transformasi TF-IDF pada teks inputan
	    tfidf_input = tfidf_vectorizer.transform([input_text])

	    # Melakukan prediksi label dengan menggunakan model Gaussian Naive Bayes
	    label_pred = gnb_model.predict(tfidf_input)

	    # Menampilkan hasil prediksi label
	    st.write("Hasil Prediksi Label:", label_pred)





    
    
elif option == 'Implementasi':	
	st.write(""" Implementasi """) #menampilkan judul halaman 
	
